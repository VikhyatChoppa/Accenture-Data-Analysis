{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5651eb5d-e82a-4169-8d61-3bb9b1e92426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ea1a4b1e-1bec-4fa6-9cc1-126650099db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 6 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   Unnamed: 0  1000 non-null   int64 \n",
      " 1   Content ID  1000 non-null   object\n",
      " 2   User ID     1000 non-null   object\n",
      " 3   Type        1000 non-null   object\n",
      " 4   Category    1000 non-null   object\n",
      " 5   URL         801 non-null    object\n",
      "dtypes: int64(1), object(5)\n",
      "memory usage: 47.0+ KB\n"
     ]
    }
   ],
   "source": [
    "Content_df = pd.read_csv(\"Content.csv\")\n",
    "Reactions_df = pd.read_csv(\"Reactions.csv\")\n",
    "Reactiontypes_df = pd.read_csv(\"ReactionTypes.csv\")\n",
    "Content_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e603fc-1f10-4edc-aca7-416230f6af33",
   "metadata": {},
   "source": [
    "##Data Cleaning operation#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "9551817b-9a9a-482e-bee4-fc972d0c8cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0      0\n",
      "Content ID      0\n",
      "User ID         0\n",
      "Type            0\n",
      "Category        0\n",
      "URL           199\n",
      "dtype: int64\n",
      "Duplicate values 0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 801 entries, 0 to 999\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Unnamed: 0    801 non-null    int64 \n",
      " 1   Content ID    801 non-null    object\n",
      " 2   Content_Type  801 non-null    object\n",
      " 3   Category      801 non-null    object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 31.3+ KB\n"
     ]
    }
   ],
   "source": [
    "print(Content_df.isnull().sum())       \n",
    "Content_df = Content_df.dropna()               \n",
    "print(\"Duplicate values\",Content_df.duplicated().sum())\n",
    "Content_df = Content_df.drop(['User ID', 'URL'], axis=1)\n",
    "\n",
    "Content_df = Content_df.rename(columns={'Type':'Content_Type'})\n",
    "Content_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a22c15bf-98ba-4c67-924d-3ac95c14df5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0       0\n",
      "Content ID       0\n",
      "User ID       3019\n",
      "Type           980\n",
      "Datetime         0\n",
      "dtype: int64\n",
      "Duplicate values 0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 22534 entries, 1 to 25552\n",
      "Data columns (total 3 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Content ID     22534 non-null  object\n",
      " 1   Reaction_Type  22534 non-null  object\n",
      " 2   Datetime       22534 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 704.2+ KB\n"
     ]
    }
   ],
   "source": [
    "print(Reactions_df.isnull().sum())       \n",
    "Reactions_df = Reactions_df.dropna()\n",
    "print(\"Duplicate values\",Reactions_df.duplicated().sum())\n",
    "Reactions_df = Reactions_df.drop(['User ID', 'Unnamed: 0' ], axis=1)\n",
    "Reactions_df = Reactions_df.rename(columns={'Type':'Reaction_Type'})\n",
    "Reactions_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3d72a502-fb3c-40bb-b4be-eab07ad32007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0    0\n",
      "Type          0\n",
      "Sentiment     0\n",
      "Score         0\n",
      "dtype: int64\n",
      "Duplicate values 0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16 entries, 0 to 15\n",
      "Data columns (total 4 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Unnamed: 0     16 non-null     int64 \n",
      " 1   Reaction_Type  16 non-null     object\n",
      " 2   Sentiment      16 non-null     object\n",
      " 3   Score          16 non-null     int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 640.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "print(Reactiontypes_df.isnull().sum())       \n",
    "Reactiontypes_df = Reactiontypes_df.dropna()\n",
    "print(\"Duplicate values\",Reactiontypes_df.duplicated().sum())\n",
    "Reactiontypes_df = Reactiontypes_df.rename(columns={'Type':'Reaction_Type'})\n",
    "\n",
    "Reactiontypes_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "3816c351-5f8d-4d51-8fb6-46ea3e9ff536",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging the dataset\n",
    "Merge_1 = pd.merge(Reactiontypes_df,Content_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9fa794d7-250b-422a-a3f6-2de48fa59ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 13 entries, 0 to 12\n",
      "Data columns (total 7 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Unnamed: 0     13 non-null     int64 \n",
      " 1   Reaction_Type  13 non-null     object\n",
      " 2   Sentiment      13 non-null     object\n",
      " 3   Score          13 non-null     int64 \n",
      " 4   Content ID     13 non-null     object\n",
      " 5   Content_Type   13 non-null     object\n",
      " 6   Category       13 non-null     object\n",
      "dtypes: int64(2), object(5)\n",
      "memory usage: 832.0+ bytes\n",
      "   Reaction_Type Sentiment  Score Content_Type         Category\n",
      "0          heart  positive     60        photo         Studying\n",
      "1           want  positive     70        photo   healthy eating\n",
      "2        disgust  negative      0        photo   healthy eating\n",
      "3           hate  negative      5        photo       technology\n",
      "4     interested  positive     30        video             food\n",
      "5           love  positive     65        video             dogs\n",
      "6     super love  positive     75        video       technology\n",
      "7        cherish  positive     70        photo           soccer\n",
      "8          adore  positive     72        photo  public speaking\n",
      "9        dislike  negative     10        audio  public speaking\n",
      "10     intrigued  positive     45          GIF           tennis\n",
      "11       peeking   neutral     35        video             food\n",
      "12        scared  negative     15        audio           travel\n"
     ]
    }
   ],
   "source": [
    "Merge_1.info()\n",
    "Merge_1 = Merge_1.drop(['Unnamed: 0','Content ID'], axis=1)\n",
    "print(Merge_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "17dd1a9f-b356-4acb-bb78-c06646ba3d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_merge = pd.merge(Merge_1,Reactions_df,on='Reaction_Type')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f2235b93-658e-438b-8362-c85bb7cba47b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 18379 entries, 0 to 18378\n",
      "Data columns (total 7 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Reaction_Type  18379 non-null  object\n",
      " 1   Sentiment      18379 non-null  object\n",
      " 2   Score          18379 non-null  int64 \n",
      " 3   Content_Type   18379 non-null  object\n",
      " 4   Category       18379 non-null  object\n",
      " 5   Content ID     18379 non-null  object\n",
      " 6   Datetime       18379 non-null  object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "final_merge.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "bf6a79af-ba75-484e-98c7-d247f6e2e2ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate values 0\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 18379 entries, 0 to 18378\n",
      "Data columns (total 7 columns):\n",
      " #   Column         Non-Null Count  Dtype \n",
      "---  ------         --------------  ----- \n",
      " 0   Reaction_Type  18379 non-null  object\n",
      " 1   Sentiment      18379 non-null  object\n",
      " 2   Score          18379 non-null  int64 \n",
      " 3   Content_Type   18379 non-null  object\n",
      " 4   Category       18379 non-null  object\n",
      " 5   Content ID     18379 non-null  object\n",
      " 6   Datetime       18379 non-null  object\n",
      "dtypes: int64(1), object(6)\n",
      "memory usage: 1.1+ MB\n",
      "      Reaction_Type Sentiment  Score Content_Type  Category  \\\n",
      "0             heart  positive     60        photo  studying   \n",
      "1             heart  positive     60        photo  studying   \n",
      "2             heart  positive     60        photo  studying   \n",
      "3             heart  positive     60        photo  studying   \n",
      "4             heart  positive     60        photo  studying   \n",
      "...             ...       ...    ...          ...       ...   \n",
      "18374        scared  negative     15        audio    travel   \n",
      "18375        scared  negative     15        audio    travel   \n",
      "18376        scared  negative     15        audio    travel   \n",
      "18377        scared  negative     15        audio    travel   \n",
      "18378        scared  negative     15        audio    travel   \n",
      "\n",
      "                                 Content ID             Datetime  \n",
      "0      97522e57-d9ab-4bd6-97bf-c24d952602d2  2021-04-11 14:29:59  \n",
      "1      9f737e0a-3cdd-4d29-9d24-753f4e3be810  2021-06-16 03:02:28  \n",
      "2      230c4e4d-70c3-461d-b42c-ec09396efb3f  2020-12-25 00:09:41  \n",
      "3      230c4e4d-70c3-461d-b42c-ec09396efb3f  2020-06-21 16:57:09  \n",
      "4      230c4e4d-70c3-461d-b42c-ec09396efb3f  2021-05-14 12:47:27  \n",
      "...                                     ...                  ...  \n",
      "18374  435007a5-6261-4d8b-b0a4-55fdc189754b  2020-09-12 23:33:40  \n",
      "18375  435007a5-6261-4d8b-b0a4-55fdc189754b  2020-06-22 06:19:10  \n",
      "18376  4e4c9690-c013-4ee7-9e66-943d8cbd27b7  2021-06-02 06:32:32  \n",
      "18377  4e4c9690-c013-4ee7-9e66-943d8cbd27b7  2021-03-20 06:36:53  \n",
      "18378  4e4c9690-c013-4ee7-9e66-943d8cbd27b7  2021-01-19 07:46:07  \n",
      "\n",
      "[18379 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Duplicate values\",final_merge.duplicated().sum())\n",
    "final_merge.isna().sum()\n",
    "final_merge['Category'] = final_merge['Category'].str.lower()\n",
    "final_merge.info()\n",
    "final_merge['Category'].unique\n",
    "print(final_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4edef5c3-ac4f-44c2-adf9-0cfd106d3c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 React_Type_Count\n",
      "Category                         \n",
      "food                         2863\n",
      "public speaking              2823\n",
      "technology                   2813\n",
      "healthy eating               2802\n",
      "studying                     1497\n",
      "                 Score_Sum\n",
      "Category                  \n",
      "public speaking     116704\n",
      "technology          111575\n",
      "healthy eating       97720\n",
      "soccer               96600\n",
      "food                 93080\n"
     ]
    }
   ],
   "source": [
    "def top_n_counts(final_merge, n):\n",
    "    return final_merge.groupby('Category') \\\n",
    "             .agg(React_Type_Count=('Reaction_Type', 'count')) \\\n",
    "             .sort_values(by='React_Type_Count', ascending=False) \\\n",
    "             .head(n)\n",
    "\n",
    "def sum_scores(final_merge):\n",
    "    return final_merge.groupby('Category') \\\n",
    "             .agg(Score_Sum=('Score', 'sum')) \\\n",
    "             .sort_values(by='Score_Sum', ascending=False) \\\n",
    "             .head()\n",
    "\n",
    "top_react_type_counts = final_merge.pipe(top_n_counts, n=5)\n",
    "top_scores = final_merge.pipe(sum_scores)\n",
    "print(top_react_type_counts)\n",
    "print(top_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ca620f11-5f8c-4431-8d21-319682cbeb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create an Excel writer object with specified encoding\n",
    "writer = pd.ExcelWriter('results.xlsx', engine='xlsxwriter', engine_kwargs={'options': {'encoding': 'utf-8-sig'}})\n",
    "\n",
    "# Write the top_react_type_counts DataFrame to a sheet named 'React_Type_Counts'\n",
    "top_react_type_counts.to_excel(writer, sheet_name='React_Type_Counts', index=False)\n",
    "\n",
    "# Write the top_scores DataFrame to a sheet named 'Scores'\n",
    "top_scores.to_excel(writer, sheet_name='Scores', index=False)\n",
    "\n",
    "# Save the Excel file\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4e131613-228d-4c29-bec6-22cc1efd9f25",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'heart'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearRegression\n\u001b[0;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m LinearRegression()\n\u001b[1;32m----> 4\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mB_train\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:648\u001b[0m, in \u001b[0;36mLinearRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    644\u001b[0m n_jobs_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs\n\u001b[0;32m    646\u001b[0m accept_sparse \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositive \u001b[38;5;28;01melse\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsr\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcoo\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m--> 648\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    652\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(\n\u001b[0;32m    653\u001b[0m     sample_weight, X, dtype\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mdtype, only_non_negative\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    654\u001b[0m )\n\u001b[0;32m    656\u001b[0m X, y, X_offset, y_offset, X_scale \u001b[38;5;241m=\u001b[39m _preprocess_data(\n\u001b[0;32m    657\u001b[0m     X,\n\u001b[0;32m    658\u001b[0m     y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    661\u001b[0m     sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m    662\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:565\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    563\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    564\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 565\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    566\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:1106\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1101\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1103\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1104\u001b[0m     )\n\u001b[1;32m-> 1106\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1108\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1109\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1110\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1112\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1113\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1114\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1115\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1116\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1119\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1120\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1122\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1124\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:879\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    877\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    878\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 879\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43m_asarray_with_order\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    881\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    882\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    883\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\_array_api.py:185\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    182\u001b[0m     xp, _ \u001b[38;5;241m=\u001b[39m get_namespace(array)\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m xp\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy.array_api\u001b[39m\u001b[38;5;124m\"\u001b[39m}:\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;66;03m# Use NumPy API to support order\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mnumpy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:2070\u001b[0m, in \u001b[0;36mNDFrame.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   2069\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__array__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dtype: npt\u001b[38;5;241m.\u001b[39mDTypeLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m-> 2070\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'heart'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd442ba3-a78f-40b9-81e0-0d504ed2bfd8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
